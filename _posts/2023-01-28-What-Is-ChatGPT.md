---
layout: post
title: "What Is ChatGPT"
excerpt: "This is an accessible introduction to explain what ChatGPT is and how it works. It uses plain language to aim at general audience without any technical background in machine learning."
date: 2023-01-28 
---

This is an accessible introduction to explain what ChatGPT is and how it works. It uses plain language to aim at general audience without any technical background in machine learning.

*OpenAI* launched a new chatbot service called *ChatGPT* in November 2022, and it has immediately attracted tremendous attention all over the world. It is said that *ChatGPT* has recorded one million users in only five days, faster than any service or product ever created before.  Meanwhile, massive compliments on its implausible smartness and amazing capabilities have started to pour in social media. Chatbots are not new to us at all as many of us have before experienced other products, such as Apple’s Siri and Amazon's Alexa. However, people are astonished this time that *ChatGPT* is clearly different from all precedent AI products ever invented. In my opinion, *ChatGPT* is a revolutionary product in technology developement and it is doomed to change our daily lives in many fundamental ways. Meanwhile, *ChatGPT* is a tipping point in our long-time pursuit for artificial intelligence (AI) and it has been significantly reshaping the landscape of AI research and development, both in acadamics and industries. 

As a long-time AI researcher and an early user of  *ChatGPT*, I shall write several blogs to talk about *ChatGPT* and its impacts on our society, particularly focusing on how it may change our current post-secondary education, as well as its influences on the AI research, especially in the areas of machine learning and natural language processing. 

Here, as the first post of a series of three blogs, I would like to use plain language to explain what *ChatGPT* is and how it works.  The technology behind *ChatGPT* is not particularly innovative on its own as all core techniques used in *ChatGPT* were invented by many people in the field over the past decades (most of them are not even with *OpenAI*). In this post, I will explain how *OpenAI* has managed to take advantage of some previously-known techniques and successfully delivered such a revolutionary and influential AI product that is doomed to change the world. Note that this introduction is non-technical, aiming at general audience without assuming any technical background. 

### **Language Models** 

First of all, *ChatGPT* mainly relies on a colossal deep neural network model as its core, often called GPT-3, which works as a large language model (LLM). Language models are not a new technique at all, and they have been around for at least 60 years. The basic idea of language models is to build a prediction model to let computers guess what word would follow a given partial sentence (or paragraph). This partial sentence (or paragraph), often called *history*, is given as input, based on which language models predict what word (or words) may have a good chance to follow the given partial sentence to eventually form a meaningful sentence at the end.  We know all words appearing in either a meaningful sentence, or a coherent paragraph, or a logical document are highly correlated. It is impossible to convey any purposeful message or composing any meaningful sentence from some random words from dictionary. Moreover, these correlated words must follow certain order to ensure the sentence is comprehensive. Of course, we often have more than one order to organize these words into a meaningful message one way or another but the order cannot be arbitrary. 

Language models are built by a typical machine learning approach: we first collect a large text corpus consisting of a large number of sentences or documents, and then  let the language model go through these sentences one by one to learn both correlation between words and the order information. After this training process is done, the learned language model will be able to predict what word may follow  any given text prompt, which may be a partial or even full sentence (or paragraph), as the language model has learned some universal information about how to organize words in natural languages. More specifically, for any given text prompt (no matter whether it is short or long),  language models can compute a probability score (between 0 and 1) for each word in dictionary to indicate how likely this word may appear right after the given prompt.  For example, given any partial English sentence, when we go through all words in dictionary, it is easy to tell that most words are impossible to appear right after it to compose a grammatically-correct, meaningful, and logical sentence. A good language model will give probability zeros (0) to these words. Of course, we usually can find a smaller number of words (than the total number of words in dictionary) which could appear right after the partial sentence with a more or less potential to form a meaningful sentence at the end. A good language model should give non-zero probability scores to these words, e.g. 0.1, 0.2, 0.35, etc. The value of each probability indicates how well this word fits in this context, the larger and the better. 

In the past, due to the limitation of computing resources as well as the amount of training corpus available to us, we must make some strong assumptions (a.k.a. *inductive biases*) in traditional language models. For example, no matter how long the partial sentence is, we only look at a few most recent words (e.g., only 1-3 words) that appear right preceding the current prediction position, namely the last 1-3 words at the end of the given partial sentence within a hypothetical look-up window. In other words, language models actually can only use the words within this look-up window to predict next word rather than the full context if the context is longer than the window, and the window can be slided to right one position at a time to continuously predict for subsequent words. In traditional language models, in order to make model size manageable, we cannot grow the look-up window over three words. Generally speaking, it is a sensible choice to use only a small number of most recent words within the look-up window for prediction if the computing resource is constrained.  The most recent words do show the strongest average correlation with the current prediction position if we consider many different partial sentences and average over all these different contexts. The fact that the average correlation is the strongest does not mean the correlation is strong for every case. For instance, if we read a long text document, in many cases, we can easily spot the reason why a particular word appears in a place is more related to another word or the other few words appearing in some quite far-away context (sometimes even a few sentences apart), rather than the immediate neighboring words.  Because of the underlying assumptions adopted in the first place, traditional language models are completely crippled to capture this type of long-distance dependency abundant in natural languages. Some earlier-generation neural networks language models improved the situation slightly, but some empirical studies have also shown that they cannot effectively make use of more than 7-8 most recent words in each context for prediction. In other wrods, their look-up windows usually can not exceed 7-8 in average. 

### **Transformer** 

The language model used by ChatGPT, called *GPT-3*, has adopted a new neural network architecture, called *transformer*, which was initially proposed by Google researchers in 2017. There is no doubt that transformer was a new architecture when it was initially proposed in 2017,  but we could actually find many similar or relevant architectures prior to it. There is no breakthrough or revolutionary idea behind the *transformer* architecture. The underlying math essentially involves certain multiplications and  manipulations of several large matrices. The transformers entail a large number of model parameters in these large matrices which need to be trained, and moreover transformers are very  computationally intensive because the computational complexity of this architecture is quadratic to both input data size and model size. That is, when its model size (or input data dimension) grows N times, the number of computation steps in transformers grow N<sup>2</sup>. In the past, it was unwise to propose or use any quadratically-complex models and researchers usually endeavored to simplify model strcuture to make their models at least linear, whose total computation steps only grows N times in above cases.  However, the researchers and engineers at Google probably had plenty of CPUs and GPUs to abuse in 2017. They first proposed to use this quadratically-complex model structure for some benchmark language translation tasks and reported some impressive performance gains. Immediately after that, this architecture is widely accepted by many researchers and engineers all over the world for more and more language-related tasks. At present, transformers are considered to be the dominant neural network model in almost all AI tasks, not just language but also speech and vision related applications. 

The large language model behind *ChatPGT*, i.e. *GPT-3*, adopts the same structure as Google’s original transformer but it is way more bigger. Comparing with *GPT-3*, the Google’s original transformer model is a peanut. Over the past few years, OpenAI has boldly scaled up the size of transformer in all possible dimensions. After several generations of development (*GPT* in 2018 and *GPT-2* in 2019), OpenAI initially released a gigantic *GPT-3* in 2020 and kept updating it until 2022. Remember that traditional language models only look at a small window of 1-3 preceding words to make prediction for next word.  *GPT-3* extends its  look-up window to cover at most 2048 words <sup>[*]</sup>.  In other words, *GPT-3* can make use of  up to 2048 words preceding the current position to predict what words may appear there. A sequence of 2048 words is pretty long, and it can easily span many sentences, many paragraphs or even several documents. Of course, *GPT-3* does not equally treat all 2048 words in the look-up window, the transformer architecture allows prediction under each different context to focus on different words or phrases  and at the same time completely ignore all other irrelevant words within the window. This is often called *attention* mechanism, which is so flexible in the transformer architecture that it can automatically adjust the language model to focus on different words or different combinations of words inside its long look-up window for different contexts. Of course, the costs associated with this powerful adaptive *attention* mechanism include:
- Computation complexity is extremely high, becoming quadratic to input data size, widow size and model size; 
- A large number of parameters are necessary to cope with a rich set of contexts in natural languages.  

### **Large Language Models: GPT-3 and ChatGPT** 

The final version of *GPT-3* contains 175 billion parameters, requiring a gigantic space of approximately 1 terabytes (10<sup>12</sup> bytes) to simply just store it. If anyone wants to make use of *GPT-3* in any way, the entire model needs to be loaded into computer memory. Even for today, not many computers or servers across the world can hold GPT-3 in memory. Obviously, even GPT-3 is made freely available to anyone, only a small fraction of users in the world can actually load it in memory and eventually take advantage of it. When training this gigantic GPT-3, OpenAI has used a large quantity of text scraped from the Internet. There are totally more than one trillion words of raw text scraped from the Internet over the past decades, which include almost all online books, articles, blogs, twits, books, advertises, reviews/comments, and so on. Basically, it covers pretty much all stuffs people have ever posted in the Web and it contains text in over 40 different languages and even a large quantity of computer programs/codes posted in the Web along with the associated  explanations and comments. It is understandable these raw crawls contain lots of noises, garbage, toxic content. OpenAI has done an excellent job to filter and clean up these raw crawls and eventually generate a high-quality subset to train *GPT-3*.  This filtered subset is still very large, consisting of 300 billions words <sup>[*]</sup>. A rough estimation shows that these words can easily make up to 2-3 millions of thick books, which is almost equivalent to the entire collection in one of those largest libraries in the world. As of today, all Wikipedia pages ever created in the Web contain only 3 billion words, which is a part of this subset but accounts for only 1%.  During the training process, in principle, we need to make the *GPT-3* model to look at each word in this subset of 300 billion words along with its preceding words (up to 2048 words) one by one and meanwhile adjust all 175 billions parameters to ensure GPT-3 remember each context in order to make a good prediction in each case.  It is not hard to imagine that this training process is extremely demanding in computing resources. OpenAI reports that each training cycle takes several months to finish in a large cluster of high-end servers consisting of thousands of top-ranking CPUs and GPUs.  We can easily estimate that only the electricity bill to run these machines for several months will go up to 1-2 millions US dollars even when it is conducted in some states with low energy cost. This does not include many other costs, such as purchasing or renting these machines, preparing and storing the one terabyte of training data, operation costs and salary expenses. Also it is normal to run several training cycles to fine-tune various settings in the training procedure to be able to deliver the best possible model.  It has been reported that the total cost for OpenAI to train *GPT-3* only once amounts to about 12 millions US dollars in average.  

Once a large language model like *GPT-3* is trained, a good use of it is to generate text under a text prompt. In this case, the given prompt is fed to *GPT-3* as input to compute probability scores for all words of following that prompt as the first word in a reply. It normally produces probability zeros for all incoherent words but non-zero probabilities only for a handful of words that can potentially fit there, e.g. 0.3 for word *A*, 0.25 for *B*, 0.13 for *C*, etc. According to these probabilities, we randomly sample one as the first word in the reply, i.e. 30% of chance to be *A*, 25% of chance as *B*, 13% as *C* and so on. Once the first word in the reply is determined, we continuously feed the original prompt plus the first word in the reply to *GPT-3* to repeat the same computation to determine the second word for the reply. This process continues until a special termination symbol is finally sampled. This explains why *Chat-GPT* can generate different replies in various trials even under the same prompt, sometimes long sometimes short. As long as the large language model is well trained over enough training corpus, all these replies will be fluent and look plausible under the provided prompt.

In addition to *GPT-3*, *ChatGPT* also requires another training step to learn another smaller model to guide *GPT-3* to generate the best possible responses under many common prompts. This step requires to hire many human annotators to read different responses generated by *GPT-3* under different prompts and manually label whether each particular response is sufficiently good for that prompt. All these human labels are used to train another model to guide  *ChatGPT* to produce only the best responses under each prompt, which is often called as aligning language models to follow instructions. 

Apparently, the lofty cost involved has essentially wiped out all academic teams and smaller industrial players from this large language model business. This has become a game of several big billion-dollar industrial players. In addition to OpenAI (teamed up with Microsoft), several other large tech companies, such as Google, Meta, have been actively exploring large language models in the past few years. However, for the first time, *ChatGPT* has convinced many AI researchers and partitioners (including myself) that large language models are a promising direction towards artificial general intelligence (AGI). For the first time, many serious AI partitioners (not those journalists and sci-fic writers) have started to believe that AGI is not only feasible but also just around the corner.  

[*] *To be exact, the window contains 2048 tokens, or word fragments. Let’s ignore this minor technical detail here.*